{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Building a Rule-based Sentiment Classifier**\n",
    "This notebook is an attempt to build a rule-based sentiment classifier. It will take in a text X and return a label of \"1\" if the sentiment of the text is positive, \"-1\" if the sentiment of the text is negative, and \"0\" if the sentiment of the text is neutral. You can test the accuracy of your classifier on the Stanford Sentiment Treebank by running the notebook all the way to end.\n",
    "\n",
    "The final way the classifier decides whether to assign a positive, negative, or neutral label is by calculating the dot product feature_weights * extract_features(X), and if the value is greater than zero, return 1, less than zero return -1, and if exactly zero return 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(x: str) -> dict[str, float]:\n",
    "    features = {}\n",
    "    x_split = x.split(' ')\n",
    "    \n",
    "    good_words = ['love', 'good', 'nice', 'great', 'enjoy', 'enjoyed']\n",
    "    bad_words = ['hate', 'bad', 'terrible', 'disappointing', 'sad', 'lost', 'angry']\n",
    "    \n",
    "    for x_words in x_split:\n",
    "        if x_words in good_words:\n",
    "            features['good_word_count'] = features.get('good_word_count', 0) + 1\n",
    "        if x_words in bad_words:\n",
    "            features['bad_word_count'] = features.get('bad_word_count', 0) + 1 \n",
    "            \n",
    "    features['bias'] = 1\n",
    "    \n",
    "    return features\n",
    "    \n",
    "feature_weights = {'good_word_count':1.0,'bad_word_count': -1.0, 'bias':0.5 }\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good_word_count': 2, 'bad_word_count': 1, 'bias': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features(\"I love to play football because it is nice and sad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">From the example above, we can see that the function successfully extracts the number of good and bad words in the input (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reading the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def read_xy_data(filename: str) -> tuple[list[str], list[int]]:\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            label, text = line.strip().split(' ||| ')\n",
    "            x_data.append(text)\n",
    "            y_data.append(int(label))\n",
    "    return x_data, y_data\n",
    "\n",
    "x_train, y_train = read_xy_data('./data/train.txt')\n",
    "x_test, y_test = read_xy_data('./data/test.txt') \n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Run the Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(x: str) -> int:\n",
    "    score = 0\n",
    "\n",
    "    for feat_name, feat_value in extract_features(x).items():\n",
    "        score = score + feat_value * feature_weights.get(feat_name, 0)\n",
    "        \n",
    "    if score > 1:\n",
    "        return 1\n",
    "    elif score < -1:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_classifier(\"I hate to play football because it is nice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculate Accuracy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(x_data: list[str], y_data: list[int]) -> float:\n",
    "    total_number = 0\n",
    "    correct_number = 0\n",
    "    for x, y in zip(x_data, y_data, strict=True):\n",
    "        y_pred = run_classifier(x)\n",
    "        total_number += 1\n",
    "        if y == y_pred:\n",
    "            correct_number += 1\n",
    "    return correct_number / float(total_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 389, 1: 909, -1: 912}\n"
     ]
    }
   ],
   "source": [
    "label_count = {}\n",
    "for y in y_test:\n",
    "    if y not in label_count:\n",
    "        label_count[y] = 0\n",
    "    label_count[y] += 1\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.21594101123595505\n",
      "Dev/test accuracy: 0.19411764705882353\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calculate_accuracy(x_train, y_train)\n",
    "test_accuracy = calculate_accuracy(x_test, y_test)\n",
    "print(f'Train accuracy: {train_accuracy}')\n",
    "print(f'Dev/test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ERROR ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def find_errors(x_data, y_data):\n",
    "    error_ids = []\n",
    "    y_preds = []\n",
    "    for i, (x, y) in enumerate(zip(x_data, y_data)):\n",
    "        y_preds.append(run_classifier(x))\n",
    "        if y != y_preds[-1]:\n",
    "            error_ids.append(i)\n",
    "    for _ in range(5):\n",
    "        my_id = random.choice(error_ids)\n",
    "        x, y, y_pred = x_data[my_id], y_data[my_id], y_preds[my_id]\n",
    "        print(f'{x}\\ntrue label: {y}\\npredicted label: {y_pred}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard-core slasher aficionados will find things to like ... but overall the Halloween series has lost its edge .\n",
      "true label: -1\n",
      "predicted label: 0\n",
      "\n",
      "A thoroughly awful movie -- dumb , narratively chaotic , visually sloppy ... a weird amalgam of ` The Thing ' and a geriatric ` Scream . '\n",
      "true label: -1\n",
      "predicted label: 0\n",
      "\n",
      "This toothless Dog , already on cable , loses all bite on the big screen .\n",
      "true label: -1\n",
      "predicted label: 0\n",
      "\n",
      "The Cat 's Meow marks a return to form for director Peter Bogdanovich ...\n",
      "true label: 1\n",
      "predicted label: 0\n",
      "\n",
      "Off the Hook is overlong and not well-acted , but credit writer-producer-director Adam Watstein with finishing it at all .\n",
      "true label: -1\n",
      "predicted label: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_errors(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SIMPLE BOW EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(x: str) -> dict[str, float]:\n",
    "    features = {}\n",
    "    x_split = x.split(' ')\n",
    "    for x in x_split:\n",
    "        features[x] = features.get(x, 0) + 1.0\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 1.0, 'am': 1.0, 'a': 1.0, 'good': 1.0, 'boy': 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "extract_features(\"I am a good boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_weights = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inference Code**\n",
    "How we run the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(features: dict[str, float]) -> int:\n",
    "    score = 0\n",
    "    for feat_name, feat_value in features.items():\n",
    "        score = score + feat_value * feature_weights.get(feat_name, 0)\n",
    "    if score > 0:\n",
    "        return 1\n",
    "    elif score < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "run_classifier({'I': 1.0, 'am': 1.0, 'a': 1.0, 'good': 1.0, 'boy': 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training Code**\n",
    "> Learn the weights of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 8544/8544 [00:00<00:00, 21190.18it/s]\n",
      "Epoch 2: 100%|██████████| 8544/8544 [00:00<00:00, 71243.81it/s]\n",
      "Epoch 3: 100%|██████████| 8544/8544 [00:00<00:00, 47761.52it/s]\n",
      "Epoch 4: 100%|██████████| 8544/8544 [00:00<00:00, 52450.59it/s]\n",
      "Epoch 5: 100%|██████████| 8544/8544 [00:00<00:00, 49996.70it/s]\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    # Shuffle the order of the data\n",
    "    data_ids = list(range(len(x_train)))\n",
    "    random.shuffle(data_ids)\n",
    "    # Run over all data points\n",
    "    for data_id in tqdm.tqdm(data_ids, desc=f'Epoch {epoch}'):\n",
    "        x = x_train[data_id]\n",
    "        y = y_train[data_id]\n",
    "        # We will skip neutral examples\n",
    "        if y == 0:    \n",
    "            continue\n",
    "        # Make a prediction\n",
    "        features = extract_features(x)\n",
    "        predicted_y = run_classifier(features)\n",
    "        # Update the weights if the prediction is wrong\n",
    "        if predicted_y != y:\n",
    "            for feature in features:\n",
    "                feature_weights[feature] = feature_weights.get(feature, 0) + y * features[feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation Code**\n",
    "How we evaluate the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(x_data: list[str], y_data: list[int]) -> float:\n",
    "    total_number = 0\n",
    "    correct_number = 0\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        y_pred = run_classifier(extract_features(x))\n",
    "        total_number += 1\n",
    "        if y == y_pred:\n",
    "            correct_number += 1\n",
    "    return correct_number / float(total_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 389, 1: 909, -1: 912}\n"
     ]
    }
   ],
   "source": [
    "label_count = {}\n",
    "for y in y_test:\n",
    "    if y not in label_count:\n",
    "        label_count[y] = 0\n",
    "    label_count[y] += 1\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7560861423220974\n",
      "Dev/test accuracy: 0.6135746606334842\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calculate_accuracy(x_train, y_train)\n",
    "test_accuracy = calculate_accuracy(x_test, y_test)\n",
    "print(f'Train accuracy: {train_accuracy}')\n",
    "print(f'Dev/test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Error Analysis**\n",
    "An important part of improving any system is figuring out where it goes wrong. The following two functions allow you to randomly observe some mistaken examples, which may help you improve the classifier. Feel free to write more sophisticated methods for error analysis as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You wo n't believe much of it , but you will laugh at the audacity , at the who 's who casting and the sheer insanity of it all .\n",
      "true label: 0\n",
      "predicted label: 1\n",
      "\n",
      "Strong setup and ambitious goals fade as the film descends into unsophisticated scare tactics and B-film thuggery .\n",
      "true label: -1\n",
      "predicted label: 1\n",
      "\n",
      "What will , most likely , turn out to be the most repellent movie of 2002 .\n",
      "true label: -1\n",
      "predicted label: 1\n",
      "\n",
      "For all its highfalutin title and corkscrew narrative , the movie turns out to be not much more than a shaggy human tale .\n",
      "true label: -1\n",
      "predicted label: 1\n",
      "\n",
      "With a story inspired by the tumultuous surroundings of Los Angeles , where feelings of marginalization loom for every dreamer with a burst bubble , The Dogwalker has a few characters and ideas , but it never manages to put them on the same path .\n",
      "true label: -1\n",
      "predicted label: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_errors(x_data, y_data):\n",
    "    error_ids = []\n",
    "    y_preds = []\n",
    "    for i, (x, y) in enumerate(zip(x_data, y_data)):\n",
    "        y_preds.append(run_classifier(extract_features(x)))\n",
    "        if y != y_preds[-1]:\n",
    "            error_ids.append(i)\n",
    "    for _ in range(5):\n",
    "        my_id = random.choice(error_ids)\n",
    "        x, y, y_pred = x_data[my_id], y_data[my_id], y_preds[my_id]\n",
    "        print(f'{x}\\ntrue label: {y}\\npredicted label: {y_pred}\\n')\n",
    "find_errors(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
